{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erha500/IMLO-Open-Assessment/blob/main/IMLO_Open_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MGJhK0Wy6U2-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1FLWsWbK-ylG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5be2211-b727-4f2f-b737-2059dfa3b89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\erhan\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_transform_augment = v2.Compose([\n",
        "    v2.RandomResizedCrop([224,224], scale=[0.75,1.0], ratio=[1.0,1.0]),\n",
        "    #v2.RandomPerspective(),\n",
        "    v2.RandomHorizontalFlip(0.5),\n",
        "    v2.RandomRotation(20),\n",
        "    #v2.RandomVerticalFlip(0.5)\n",
        "    #v2.RandomAutocontrast(),\n",
        "    #v2.GaussianBlur(kernel_size=3),\n",
        "    v2.ToTensor(),\n",
        "    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_transform = v2.Compose([\n",
        "  v2.Resize([224,224]),\n",
        "  v2.ToTensor(),\n",
        "  v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "test_transform = v2.Compose([\n",
        "  v2.Resize([224,224]),\n",
        "  v2.ToTensor(),\n",
        "  v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWVmTrcs9QGM"
      },
      "source": [
        "Downloading Flowers102 dataset from datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2AG1ERmI9WXU"
      },
      "outputs": [],
      "source": [
        "training_data_augment = datasets.Flowers102(root=\"dataset\", split=\"train\", transform=train_transform_augment, download=True)\n",
        "\n",
        "training_data = datasets.Flowers102(root=\"dataset\", split=\"train\", transform=train_transform, download=True)\n",
        "\n",
        "val_data = datasets.Flowers102(root=\"dataset\", split=\"val\", transform=test_transform, download=True)\n",
        "\n",
        "test_data = datasets.Flowers102(root=\"dataset\", split=\"test\", transform=test_transform, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EiOOyavq-T-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bed6a4e-97dd-49ec-a1e3-cdc6a84d7895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 3, 224, 224])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "#Create data loaders\n",
        "train_dataloader = DataLoader(training_data_augment, batch_size = batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3fL6F9TLEdDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd2dee4-8a47-49f8-fbe9-8832afaeee07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): ReLU()\n",
            "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU()\n",
            "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU()\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU()\n",
            "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (18): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (20): ReLU()\n",
            "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (22): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (23): Flatten(start_dim=1, end_dim=-1)\n",
            "    (24): Linear(in_features=6272, out_features=1024, bias=True)\n",
            "    (25): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU()\n",
            "    (27): Dropout(p=0.05, inplace=False)\n",
            "    (28): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (29): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (30): ReLU()\n",
            "    (31): Dropout(p=0.05, inplace=False)\n",
            "    (32): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (33): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (34): ReLU()\n",
            "    (35): Dropout(p=0.05, inplace=False)\n",
            "    (36): Linear(in_features=256, out_features=102, bias=True)\n",
            "    (37): BatchNorm1d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (38): ReLU()\n",
            "    (39): Dropout(p=0.05, inplace=False)\n",
            "    (40): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "#Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "      #Convolutional layers\n",
        "      nn.Conv2d(3, 32, kernel_size=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32,32, kernel_size=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2, stride=2),\n",
        "      nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2, stride=2),\n",
        "      nn.AvgPool2d(2, stride=2),\n",
        "\n",
        "      #Linear layers\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(128 * 7 * 7, 1024),\n",
        "      nn.BatchNorm1d(1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.05),\n",
        "      nn.Linear(1024, 512),\n",
        "      nn.BatchNorm1d(512),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.05),\n",
        "      nn.Linear(512,256),\n",
        "      nn.BatchNorm1d(256),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.05),\n",
        "      nn.Linear(256,102),\n",
        "      nn.BatchNorm1d(102),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.05),\n",
        "      nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9IvJsfvwE0VG"
      },
      "outputs": [],
      "source": [
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = nn.NLLLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01) #weight decay is L2 regularisation\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "o4bMoWUAE1kO"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    #Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    #Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 5 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "y4Mf49itE3A-"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct) :>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    accuracy = 100 * correct\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Xr7VD7E3E5y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9fbf37-f3d1-44fa-c0d8-06e9740955e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            " ---------------------------------------\n",
            "loss: 4.891363 [   64/ 1020]\n",
            "loss: 4.407516 [  384/ 1020]\n",
            "loss: 4.370816 [  704/ 1020]\n",
            "loss: 4.012832 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 1.2%, Avg loss: 4.604004 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 2 \n",
            " ---------------------------------------\n",
            "loss: 3.618779 [   64/ 1020]\n",
            "loss: 3.897032 [  384/ 1020]\n",
            "loss: 3.904202 [  704/ 1020]\n",
            "loss: 4.017531 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 11.4%, Avg loss: 3.935567 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 3 \n",
            " ---------------------------------------\n",
            "loss: 3.573861 [   64/ 1020]\n",
            "loss: 3.273135 [  384/ 1020]\n",
            "loss: 3.336215 [  704/ 1020]\n",
            "loss: 3.208517 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 19.6%, Avg loss: 3.677950 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 4 \n",
            " ---------------------------------------\n",
            "loss: 3.379833 [   64/ 1020]\n",
            "loss: 2.950572 [  384/ 1020]\n",
            "loss: 3.118651 [  704/ 1020]\n",
            "loss: 3.281830 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 22.2%, Avg loss: 3.535559 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 5 \n",
            " ---------------------------------------\n",
            "loss: 2.862838 [   64/ 1020]\n",
            "loss: 2.632088 [  384/ 1020]\n",
            "loss: 2.947292 [  704/ 1020]\n",
            "loss: 2.935601 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 21.7%, Avg loss: 3.569802 \n",
            "\n",
            "Epoch 6 \n",
            " ---------------------------------------\n",
            "loss: 2.507688 [   64/ 1020]\n",
            "loss: 2.671051 [  384/ 1020]\n",
            "loss: 2.802328 [  704/ 1020]\n",
            "loss: 2.466231 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 27.7%, Avg loss: 3.275295 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 7 \n",
            " ---------------------------------------\n",
            "loss: 2.216764 [   64/ 1020]\n",
            "loss: 2.220257 [  384/ 1020]\n",
            "loss: 2.244926 [  704/ 1020]\n",
            "loss: 2.373443 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 22.5%, Avg loss: 3.432066 \n",
            "\n",
            "Epoch 8 \n",
            " ---------------------------------------\n",
            "loss: 2.226303 [   64/ 1020]\n",
            "loss: 2.131945 [  384/ 1020]\n",
            "loss: 2.605938 [  704/ 1020]\n",
            "loss: 2.375542 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 29.3%, Avg loss: 3.185228 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 9 \n",
            " ---------------------------------------\n",
            "loss: 1.784353 [   64/ 1020]\n",
            "loss: 2.128746 [  384/ 1020]\n",
            "loss: 2.329952 [  704/ 1020]\n",
            "loss: 2.005210 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 28.9%, Avg loss: 3.316547 \n",
            "\n",
            "Epoch 10 \n",
            " ---------------------------------------\n",
            "loss: 1.871790 [   64/ 1020]\n",
            "loss: 1.875337 [  384/ 1020]\n",
            "loss: 2.201985 [  704/ 1020]\n",
            "loss: 2.332435 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 3.167182 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 11 \n",
            " ---------------------------------------\n",
            "loss: 1.780866 [   64/ 1020]\n",
            "loss: 2.290750 [  384/ 1020]\n",
            "loss: 2.382716 [  704/ 1020]\n",
            "loss: 2.036330 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 28.6%, Avg loss: 3.206675 \n",
            "\n",
            "Epoch 12 \n",
            " ---------------------------------------\n",
            "loss: 1.560773 [   64/ 1020]\n",
            "loss: 1.821270 [  384/ 1020]\n",
            "loss: 1.898942 [  704/ 1020]\n",
            "loss: 2.104943 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 31.8%, Avg loss: 3.082760 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 13 \n",
            " ---------------------------------------\n",
            "loss: 1.540379 [   64/ 1020]\n",
            "loss: 1.634069 [  384/ 1020]\n",
            "loss: 1.398390 [  704/ 1020]\n",
            "loss: 1.675502 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 3.077723 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 14 \n",
            " ---------------------------------------\n",
            "loss: 1.520929 [   64/ 1020]\n",
            "loss: 1.616338 [  384/ 1020]\n",
            "loss: 1.738049 [  704/ 1020]\n",
            "loss: 2.012872 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 35.9%, Avg loss: 2.964463 \n",
            "\n",
            "Saved model at current state\n",
            "Epoch 15 \n",
            " ---------------------------------------\n",
            "loss: 1.646501 [   64/ 1020]\n",
            "loss: 1.647939 [  384/ 1020]\n",
            "loss: 1.750418 [  704/ 1020]\n",
            "loss: 1.866911 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 32.9%, Avg loss: 3.150184 \n",
            "\n",
            "Epoch 16 \n",
            " ---------------------------------------\n",
            "loss: 1.496690 [   64/ 1020]\n",
            "loss: 1.492576 [  384/ 1020]\n",
            "loss: 1.552068 [  704/ 1020]\n",
            "loss: 1.634420 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 29.6%, Avg loss: 3.172530 \n",
            "\n",
            "Epoch 17 \n",
            " ---------------------------------------\n",
            "loss: 1.415430 [   64/ 1020]\n",
            "loss: 1.420041 [  384/ 1020]\n",
            "loss: 1.556912 [  704/ 1020]\n",
            "loss: 1.693624 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 35.6%, Avg loss: 2.968698 \n",
            "\n",
            "Epoch 18 \n",
            " ---------------------------------------\n",
            "loss: 1.175222 [   64/ 1020]\n",
            "loss: 1.440816 [  384/ 1020]\n",
            "loss: 1.249468 [  704/ 1020]\n",
            "loss: 1.600520 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 35.8%, Avg loss: 2.954096 \n",
            "\n",
            "Epoch 19 \n",
            " ---------------------------------------\n",
            "loss: 1.313968 [   64/ 1020]\n",
            "loss: 1.147025 [  384/ 1020]\n",
            "loss: 1.609965 [  704/ 1020]\n",
            "loss: 1.243862 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 3.163345 \n",
            "\n",
            "Epoch 20 \n",
            " ---------------------------------------\n",
            "loss: 0.994935 [   64/ 1020]\n",
            "loss: 1.211878 [  384/ 1020]\n",
            "loss: 1.120045 [  704/ 1020]\n",
            "loss: 1.446368 [  960/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 33.1%, Avg loss: 3.033867 \n",
            "\n",
            "Stopping early\n",
            "Testing -----------------------\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 3.069001 \n",
            "\n",
            "Done!!!!\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "\n",
        "best_accuracy = 0\n",
        "patience = 5\n",
        "triggers = 0\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1} \\n ---------------------------------------\")\n",
        "\n",
        "  \"\"\"#train on original data\n",
        "  train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\"\"\"\n",
        "\n",
        "  #train on augmented data\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "  #testing on evaluation data\n",
        "  accuracy = test(val_dataloader, model, loss_fn)\n",
        "\n",
        "  if accuracy > best_accuracy:\n",
        "    best_accuracy = accuracy\n",
        "    triggers = 0\n",
        "    torch.save(model.state_dict(), \"model.pt\")\n",
        "    print(\"Saved model at current state\")\n",
        "  else:\n",
        "    triggers += 1\n",
        "\n",
        "  if triggers > patience:\n",
        "    print(\"Stopping early\")\n",
        "    break\n",
        "\n",
        "print(\"Testing -----------------------\")\n",
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "model.eval()\n",
        "test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!!!!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPpA4HdV5BEByEUs0EB/se5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}